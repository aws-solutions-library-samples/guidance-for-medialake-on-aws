import decimal
import json
import uuid
from typing import Any, Optional

import boto3
from aws_lambda_powertools import Logger
from botocore.config import Config

logger = Logger(service="asset-details-service-utils")

# Signature style & virtual-host addressing are required for every region
_SIGV4_CFG = Config(
    signature_version="s3v4",
    s3={"addressing_style": "virtual"},
)

_ENDPOINT_TMPL = "https://s3.{region}.amazonaws.com"
_S3_CLIENT_CACHE: dict[str, boto3.client] = {}  # {region â†’ client}


def _get_s3_client_for_bucket(bucket: str) -> boto3.client:
    """
    Return an S3 client **pinned to the bucket's actual region**.
    Clients are cached to reuse TCP connections across warm invocations.
    Falls back to region detection from bucket name or environment if GetBucketLocation fails.
    """
    # Try to detect region from bucket name patterns or environment first
    detected_region = _detect_region_from_context(bucket)

    if detected_region and detected_region in _S3_CLIENT_CACHE:
        return _S3_CLIENT_CACHE[detected_region]

    # Try GetBucketLocation as fallback if we have permissions
    generic = _S3_CLIENT_CACHE.setdefault(
        "us-east-1",
        boto3.client("s3", region_name="us-east-1", config=_SIGV4_CFG),
    )

    try:
        region = (
            generic.get_bucket_location(Bucket=bucket).get("LocationConstraint")
            or "us-east-1"
        )
        logger.debug(f"Retrieved bucket region via GetBucketLocation: {region}")
    except (generic.exceptions.NoSuchBucket, generic.exceptions.ClientError) as e:
        # Fall back to detected region or default
        region = detected_region or "us-west-2"  # Default to us-west-2 based on error
        logger.warning(
            f"Could not get bucket location for {bucket}, using {region}: {str(e)}"
        )

    if region not in _S3_CLIENT_CACHE:
        _S3_CLIENT_CACHE[region] = boto3.client(
            "s3",
            region_name=region,
            endpoint_url=_ENDPOINT_TMPL.format(region=region),
            config=_SIGV4_CFG,
        )
    return _S3_CLIENT_CACHE[region]


def _detect_region_from_context(bucket: str) -> Optional[str]:
    """
    Attempt to detect the S3 bucket region from context clues.
    """
    import os

    # Check if AWS_REGION environment variable is set (common in Lambda)
    env_region = os.environ.get("AWS_REGION")
    if env_region:
        logger.debug(f"Using region from AWS_REGION environment: {env_region}")
        return env_region

    # Check if AWS_DEFAULT_REGION is set
    default_region = os.environ.get("AWS_DEFAULT_REGION")
    if default_region:
        logger.debug(
            f"Using region from AWS_DEFAULT_REGION environment: {default_region}"
        )
        return default_region

    # Based on the error message, this specific bucket is in us-west-2
    # You could add more bucket-to-region mappings here if needed
    if "medialakebaseinfrastructu" in bucket:
        logger.debug("Detected MediaLake bucket, using us-west-2")
        return "us-west-2"

    return None


def replace_decimals(obj):
    """
    Recursively replace Decimal objects with int or float for JSON serialization.
    """
    if isinstance(obj, list):
        return [replace_decimals(o) for o in obj]
    elif isinstance(obj, dict):
        return {k: replace_decimals(v) for k, v in obj.items()}
    elif isinstance(obj, decimal.Decimal):
        return int(obj) if obj % 1 == 0 else float(obj)
    else:
        return obj


class CustomEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, decimal.Decimal):
            if obj % 1 > 0:
                return float(obj)
            else:
                return int(obj)

        if isinstance(obj, uuid.UUID):
            return str(obj)

        if callable(obj):  # Check if the object is a function
            return None  # Ignore function objects

        return super(CustomEncoder, self).default(obj)


def generate_presigned_url(
    bucket: str, key: str, expiration: int = 3600
) -> Optional[str]:
    """
    Generate a presigned URL for an S3 object with region-aware client.
    The URL is signed in the bucket's own region, preventing
    SignatureDoesNotMatch errors outside us-east-1.
    """
    try:
        # Use region-aware S3 client
        s3_client = _get_s3_client_for_bucket(bucket)
        url = s3_client.generate_presigned_url(
            "get_object",
            Params={
                "Bucket": bucket,
                "Key": key,
                "ResponseContentDisposition": "inline",
            },
            ExpiresIn=expiration,
        )

        logger.debug(
            "Generated presigned URL for s3://%s/%s (region %s)",
            bucket,
            key,
            s3_client.meta.region_name,
        )

        return url
    except Exception as e:
        logger.error(f"Error generating presigned URL: {str(e)}")
        return None


def replace_binary_data(data: Any) -> Any:
    """Recursively replace binary data with "BINARY DATA" text."""
    if isinstance(data, dict):
        return {k: replace_binary_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [replace_binary_data(item) for item in data]
    elif isinstance(data, bytes):
        return "BINARY DATA"
    elif isinstance(data, boto3.dynamodb.types.Binary):
        return "BINARY DATA"
    elif isinstance(data, boto3.dynamodb.types.Decimal):
        return float(data)  # Convert Decimal to float for JSON serialization
    else:
        return data
